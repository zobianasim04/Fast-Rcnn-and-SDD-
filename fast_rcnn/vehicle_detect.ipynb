{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO annotations saved to trafic_data/valid/coco_annotations_valid.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "# Paths\n",
    "IMAGE_DIR = \"trafic_data/valid/images\"  # Folder containing images\n",
    "YOLO_LABELS_DIR = \"trafic_data/valid/labels\"  # Folder with YOLO .txt files\n",
    "OUTPUT_JSON = \"trafic_data/valid/coco_annotations_valid.json\"\n",
    "\n",
    "# Define COCO structure\n",
    "coco = {\n",
    "    \"images\": [],\n",
    "    \"annotations\": [],\n",
    "    \"categories\": []\n",
    "}\n",
    "\n",
    "# Class Mapping\n",
    "class_mapping = {\n",
    "    1: \"ambulance\",\n",
    "    2: \"army vehicle\",\n",
    "    3: \"auto rickshaw\",\n",
    "    4: \"bicycle\",\n",
    "    5: \"bus\",\n",
    "    6: \"car\",\n",
    "    7: \"garbagevan\",\n",
    "    8: \"human hauler\",\n",
    "    9: \"minibus\",\n",
    "    10: \"minivan\",\n",
    "    11: \"motorbike\",\n",
    "    12: \"pickup\",\n",
    "    13: \"policecar\",\n",
    "    14: \"rickshaw\",\n",
    "    15: \"scooter\",\n",
    "    16: \"suv\",\n",
    "    17: \"taxi\",\n",
    "    18: \"three wheelers -CNG-\",\n",
    "    19: \"truck\",\n",
    "    20: \"van\",\n",
    "    21: \"wheelbarrow\"\n",
    "}\n",
    "\n",
    "# Ensure COCO category IDs start from 1\n",
    "category_id_map = {k: k+1 for k in class_mapping.keys()}\n",
    "\n",
    "# Add categories to COCO JSON\n",
    "for class_id, class_name in class_mapping.items():\n",
    "    coco[\"categories\"].append({\n",
    "        \"id\": category_id_map[class_id],\n",
    "        \"name\": class_name,\n",
    "        \"supercategory\": \"vehicle\"\n",
    "    })\n",
    "\n",
    "# Process each image\n",
    "image_id = 0\n",
    "annotation_id = 0\n",
    "\n",
    "for label_file in glob.glob(os.path.join(YOLO_LABELS_DIR, \"*.txt\")):\n",
    "    image_filename = os.path.splitext(os.path.basename(label_file))[0] + \".jpg\"\n",
    "    image_path = os.path.join(IMAGE_DIR, image_filename)\n",
    "\n",
    "    # Check if image exists\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Skipping {image_filename}, image not found!\")\n",
    "        continue\n",
    "\n",
    "    # Get image size\n",
    "    with Image.open(image_path) as img:\n",
    "        width, height = img.size\n",
    "\n",
    "    # Add image entry\n",
    "    coco[\"images\"].append({\n",
    "        \"id\": image_id,\n",
    "        \"file_name\": image_filename,\n",
    "        \"width\": width,\n",
    "        \"height\": height\n",
    "    })\n",
    "\n",
    "    # Read YOLO annotations\n",
    "    with open(label_file, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            parts = line.strip().split()\n",
    "            class_id = int(parts[0])  # YOLO class ID starts from 0\n",
    "\n",
    "            # Ensure class_id exists in category_id_map\n",
    "            if class_id not in category_id_map:\n",
    "                print(f\"Skipping unknown class ID {class_id} in {label_file}\")\n",
    "                continue\n",
    "\n",
    "            x_center, y_center, w, h = map(float, parts[1:])\n",
    "\n",
    "            # Convert to absolute coordinates\n",
    "            x_min = (x_center - w / 2) * width\n",
    "            y_min = (y_center - h / 2) * height\n",
    "            box_width = w * width\n",
    "            box_height = h * height\n",
    "\n",
    "            # Add annotation entry\n",
    "            coco[\"annotations\"].append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": category_id_map[class_id],\n",
    "                \"bbox\": [x_min, y_min, box_width, box_height],\n",
    "                \"area\": box_width * box_height,\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "            annotation_id += 1\n",
    "\n",
    "    image_id += 1\n",
    "\n",
    "# Save COCO JSON\n",
    "with open(OUTPUT_JSON, \"w\") as f:\n",
    "    json.dump(coco, f, indent=4)\n",
    "\n",
    "print(f\"COCO annotations saved to {OUTPUT_JSON}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping unknown class ID 0 in trafic_data/train/labels\\07_jpg.rf.8447b11632c1b63ab0e127f16625e0d2.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\09_jpg.rf.42406b1c067f04bf73349bd75b2e3fa8.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\12_jpg.rf.ac83fcb8cc8c8bbc5587c2e6881e3d4a.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\18_jpg.rf.dd32e1abd7df904495008978ddc32583.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\217_jpg.rf.679ec4f07c8bb1ee88470cf506d51788.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\68_jpg.rf.545f6cbb65441eda77d15c001778971a.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\78_jpg.rf.93edbe881bc016825faa8e839f78774c.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\80_jpg.rf.0f3e8464629e6cb01c7a1cdd5b70598b.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\85_jpg.rf.23d754dcb81727f702299ae5f1eb8f76.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\85_jpg.rf.23d754dcb81727f702299ae5f1eb8f76.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\86_JPG.rf.5a7842e85d9ca093ba4ffe02994d3eb7.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\98_jpg.rf.917d2004a976043da04d347e5a79a4c2.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Asraf_05_jpg.rf.db55b6225e6593e1880d663266e4db0b.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Asraf_06_jpg.rf.72f4c46259f5cdad6059f6fce2ad22c3.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Asraf_33_jpg.rf.16728d20183f8482dfa14e838c5efb1f.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Asraf_34_jpg.rf.a160750a762a59eb630ac2ea9e06212d.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Asraf_40_jpg.rf.a7440a2b72eea93717f367d5416dfa3e.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Dipto_-187_jpg.rf.ca4815726740326cfa83ab2faf995160.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Dipto_-232_jpg.rf.dd8ec8ccc2c3420f6aab917ad07d7e21.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Dipto_-233_jpg.rf.dca68c0fec48aa061d42205dd4a2cb07.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Dipto_-42_jpg.rf.53a32cb2ca31a4da053c6e548a4b5efc.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Dipto_351_jpg.rf.dd0c7ef80c5b8956e417fedc3a474ce3.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Dipto_355_jpg.rf.29e91a850ec68ad844f4ad761c33901f.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Dipto_358_jpg.rf.84eb20f88d52386f15e1e8f0f061af29.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Dipto_362_jpg.rf.60320993ee5fbb5361240924e1a29b19.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Dipto_363_jpg.rf.2764a7014d11b5524af004a56fbb0b27.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Dipto_364_jpg.rf.63051b5f1f8f4acf7c7c81d2b1df817a.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Dipto_365_jpg.rf.9fe9366129ec3c83d627b92ab325fde3.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Dipto_366_jpg.rf.d542739033501bf62851e81928425586.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Dipto_754_jpg.rf.c5fad55132150d25090aedba596d4ab6.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Navid_118_jpg.rf.2f6462d0be82bdfbd25811b03c2c29ac.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Navid_541_jpg.rf.cfaf543e3165481bb956279b438382b3.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Navid_542_jpg.rf.4359d93a0961ac9d0633e527c4e37b12.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Navid_543_jpg.rf.c47ad73393b43098aeb6f39e8a127be3.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Navid_544_jpg.rf.4a1889ef0c19b070562819bcc46c63a2.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Navid_643_JPG.rf.7901132985a182ee20eec3f25f19cfd9.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Navid_644_JPG.rf.4c3ce49accdab2560682eda38c13c0cc.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Navid_645_JPG.rf.90304481c0bc170020ede8ec34f91cc5.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Navid_677_jpg.rf.4f354af9bc4e8572eebf4485d29581e5.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Navid_678_jpg.rf.b6c3b11db514e98f36338e3c3fa932b5.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Navid_679_jpg.rf.29d054f0fd604051be6eac1c6bd1ea06.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Navid_680_jpg.rf.3306d80bc04e3af0061475b608fd5cd3.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Navid_747_jpg.rf.ae736d6630ce5ac40663178ea2c8beda.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Navid_748_jpg.rf.01b405ffff77d2f274a1fdc4601b3441.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Navid_797_jpg.rf.35b740cc246e681250e6cd2c64d9708b.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Numan_--10-_jpg.rf.32514b22199dd4c72bbafc3173aeaf2d.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Numan_--11-_jpg.rf.bd889565e3e0b1e6648db389bfd2b6e5.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Numan_--12-_jpg.rf.a11d3aa863feb9570cb0af3bf475d1c0.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Numan_--13-_jpg.rf.63ee1630ff8bc8539bef802af87c41ef.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Numan_--14-_jpg.rf.681a2fd3330a83a557daa7f7992940c1.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Numan_--15-_jpg.rf.1fd7032cac0a326b4d20605c3c7c182d.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Numan_--16-_jpg.rf.e1472ecb4691407495f6c696a1ec5a96.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Numan_--17-_jpg.rf.928949c9489ae3a2d11b641076a3474e.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Numan_--18-_jpg.rf.237b4298b64abba8c972983b8bf047ef.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Numan_--19-_jpg.rf.9fbe73290c8a4a7447e3bcc5eea305cc.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Numan_--20-_jpg.rf.bb5cdce815bcab35c01f7de585d6de01.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Numan_--21-_jpg.rf.157a9c86907b6a52f6704d7c23d3a47b.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Numan_--22-_jpg.rf.8b60cda7f55530ace7ccd20b728ad234.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Numan_--23-_jpg.rf.957cc45b1fc5a6af05eb81bdf80328c6.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Numan_--24-_jpg.rf.b0f41b5af46fd91145e63386d1df5969.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Numan_--25-_jpg.rf.21231bf62585d9e043db7c92e38d9d42.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Numan_--29-_jpg.rf.18d789b305b858b2ef8398f53e483005.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Numan_--32-_jpg.rf.25932601bbcdaa816ba0e4c1c500b11f.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Numan_--33-_jpg.rf.c9a4e06ebab2ec35e9c66190963b807b.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Numan_--7-_jpg.rf.02a2021585fcea66a0563aaa1d025f3b.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Numan_--8-_jpg.rf.3ae0a15bcbfb8e006d78a05337cd27b6.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Numan_--9-_jpg.rf.dfba2f24b38fca98b3fc21a73a024f52.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Numan_-384-_jpg.rf.6a2044e7d2be3693da0a17dc10c5f39c.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Numan_-385-_jpg.rf.d21e7dfa864c60cd083f109bf2678d64.txt\n",
      "Skipping unknown class ID 0 in trafic_data/train/labels\\Numan_-386-_jpg.rf.166e20b3fef307c66b8dc4ba4209f11b.txt\n",
      "COCO annotations saved to trafic_data/train/coco_annotations_train.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "# Paths\n",
    "IMAGE_DIR = \"trafic_data/train/images\"  # Folder containing images\n",
    "YOLO_LABELS_DIR = \"trafic_data/train/labels\"  # Folder with YOLO .txt files\n",
    "OUTPUT_JSON = \"trafic_data/train/coco_annotations_train.json\"\n",
    "\n",
    "# Define COCO structure\n",
    "coco = {\n",
    "    \"images\": [],\n",
    "    \"annotations\": [],\n",
    "    \"categories\": []\n",
    "}\n",
    "\n",
    "# Class Mapping\n",
    "class_mapping = {\n",
    "    1: \"ambulance\",\n",
    "    2: \"army vehicle\",\n",
    "    3: \"auto rickshaw\",\n",
    "    4: \"bicycle\",\n",
    "    5: \"bus\",\n",
    "    6: \"car\",\n",
    "    7: \"garbagevan\",\n",
    "    8: \"human hauler\",\n",
    "    9: \"minibus\",\n",
    "    10: \"minivan\",\n",
    "    11: \"motorbike\",\n",
    "    12: \"pickup\",\n",
    "    13: \"policecar\",\n",
    "    14: \"rickshaw\",\n",
    "    15: \"scooter\",\n",
    "    16: \"suv\",\n",
    "    17: \"taxi\",\n",
    "    18: \"three wheelers -CNG-\",\n",
    "    19: \"truck\",\n",
    "    20: \"van\",\n",
    "    21: \"wheelbarrow\"\n",
    "}\n",
    "\n",
    "# Ensure COCO category IDs start from 1\n",
    "category_id_map = {k: k+1 for k in class_mapping.keys()}\n",
    "\n",
    "# Add categories to COCO JSON\n",
    "for class_id, class_name in class_mapping.items():\n",
    "    coco[\"categories\"].append({\n",
    "        \"id\": category_id_map[class_id],\n",
    "        \"name\": class_name,\n",
    "        \"supercategory\": \"vehicle\"\n",
    "    })\n",
    "\n",
    "# Process each image\n",
    "image_id = 0\n",
    "annotation_id = 0\n",
    "\n",
    "for label_file in glob.glob(os.path.join(YOLO_LABELS_DIR, \"*.txt\")):\n",
    "    image_filename = os.path.splitext(os.path.basename(label_file))[0] + \".jpg\"\n",
    "    image_path = os.path.join(IMAGE_DIR, image_filename)\n",
    "\n",
    "    # Check if image exists\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Skipping {image_filename}, image not found!\")\n",
    "        continue\n",
    "\n",
    "    # Get image size\n",
    "    with Image.open(image_path) as img:\n",
    "        width, height = img.size\n",
    "\n",
    "    # Add image entry\n",
    "    coco[\"images\"].append({\n",
    "        \"id\": image_id,\n",
    "        \"file_name\": image_filename,\n",
    "        \"width\": width,\n",
    "        \"height\": height\n",
    "    })\n",
    "\n",
    "    # Read YOLO annotations\n",
    "    with open(label_file, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            parts = line.strip().split()\n",
    "            class_id = int(parts[0])  # YOLO class ID starts from 0\n",
    "\n",
    "            # Ensure class_id exists in category_id_map\n",
    "            if class_id not in category_id_map:\n",
    "                print(f\"Skipping unknown class ID {class_id} in {label_file}\")\n",
    "                continue\n",
    "\n",
    "            x_center, y_center, w, h = map(float, parts[1:])\n",
    "\n",
    "            # Convert to absolute coordinates\n",
    "            x_min = (x_center - w / 2) * width\n",
    "            y_min = (y_center - h / 2) * height\n",
    "            box_width = w * width\n",
    "            box_height = h * height\n",
    "\n",
    "            # Add annotation entry\n",
    "            coco[\"annotations\"].append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": category_id_map[class_id],\n",
    "                \"bbox\": [x_min, y_min, box_width, box_height],\n",
    "                \"area\": box_width * box_height,\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "            annotation_id += 1\n",
    "\n",
    "    image_id += 1\n",
    "\n",
    "# Save COCO JSON\n",
    "with open(OUTPUT_JSON, \"w\") as f:\n",
    "    json.dump(coco, f, indent=4)\n",
    "\n",
    "print(f\"COCO annotations saved to {OUTPUT_JSON}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.transforms import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoTransform:\n",
    "    def __call__(self,image,target):\n",
    "        image=F.to_tensor(image)\n",
    "        return image,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.22s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.61s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "def get_coco_dataset(img_dir,ann_file):\n",
    "    return CocoDetection(\n",
    "        root=img_dir,\n",
    "        annFile=ann_file,\n",
    "        transforms=CocoTransform()\n",
    "    )\n",
    "#load dataset\n",
    "train_dataset=get_coco_dataset(\n",
    "    img_dir=r\"trafic_data\\train\\images\",\n",
    "    ann_file=r\"trafic_data\\train\\coco_annotations_train.json\"\n",
    ") \n",
    "valid_dataset=get_coco_dataset(\n",
    "    img_dir=r\"trafic_data\\valid\\images\",\n",
    "    ann_file=r\"trafic_data\\valid\\coco_annotations_valid.json\"\n",
    ") \n",
    "#Dataloader\n",
    "train_loader=DataLoader(train_dataset,batch_size=4,shuffle=True,collate_fn=lambda x:tuple(zip(*x)))\n",
    "valid_loader=DataLoader(valid_dataset,batch_size=4,shuffle=False,collate_fn=lambda x:tuple(zip(*x)))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load faster rcnn from reset-50 backbone\n",
    "def get_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features=model.roi_heads.box_predictor.cls_score.in_features  #get number of input features for classifier\n",
    "    #replace pretrained head woth the new one\n",
    "    model.roi_heads.box_predictor= FastRCNNPredictor(in_features,num_classes)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ZOBIA'S WORK\\fast_rcnn\\myenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\ZOBIA'S WORK\\fast_rcnn\\myenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "num_classes=22\n",
    "model=get_model(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cpu')\n",
    "#define optimizer and learning rate scheduler\n",
    "params=[p for p in model.parameters() if p.requires_grad]\n",
    "optimizer=torch.optim.SGD(params,lr=0.005,momentum=0.9,weight_decay=0.0005)\n",
    "lr_scheduler=torch.optim.lr_scheduler.StepLR(optimizer,step_size=3,gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "    model.train()\n",
    "    for images, targets in data_loader:\n",
    "        # Move images to the device\n",
    "        images = [img.to(device) for img in images]\n",
    "\n",
    "        # Validate and process targets\n",
    "        processed_targets = []\n",
    "        valid_images = []\n",
    "        for i, target in enumerate(targets):\n",
    "            boxes = []\n",
    "            labels = []\n",
    "            for obj in target:\n",
    "                # Extract bbox\n",
    "                bbox = obj[\"bbox\"]  # Format: [x, y, width, height]\n",
    "                x, y, w, h = bbox\n",
    "\n",
    "                # Ensure the width and height are positive\n",
    "                if w > 0 and h > 0:\n",
    "                    boxes.append([x, y, x + w, y + h])  # Convert to [x_min, y_min, x_max, y_max]\n",
    "                    labels.append(obj[\"category_id\"])\n",
    "\n",
    "            # Only process if there are valid boxes\n",
    "            if boxes:\n",
    "                processed_target = {\n",
    "                    \"boxes\": torch.tensor(boxes, dtype=torch.float32).to(device),\n",
    "                    \"labels\": torch.tensor(labels, dtype=torch.int64).to(device),\n",
    "                }\n",
    "                processed_targets.append(processed_target)\n",
    "                valid_images.append(images[i])  # Add only valid images\n",
    "\n",
    "        # Skip iteration if no valid targets\n",
    "        if not processed_targets:\n",
    "            continue\n",
    "\n",
    "        # Ensure images and targets are aligned\n",
    "        images = valid_images\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, processed_targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch}] Loss: {losses.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Save the model's state dictionary after every epoch\n",
    "    model_path = f\"fasterrcnn_resnet50_epoch_{epoch + 1}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.transforms import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Load Faster R-CNN with ResNet-50 backbone\n",
    "def get_model(num_classes):\n",
    "    # Load pre-trained Faster R-CNN\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    # Get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # Replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "num_classes = 4  # Background + chair + person + table\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "# Load the trained model\n",
    "model = get_model(num_classes)\n",
    "model.load_state_dict(torch.load(\"fasterrcnn_resnet50_epoch_5.pth\"))\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n",
    "def prepare_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")  # Open image\n",
    "    image_tensor = F.to_tensor(image).unsqueeze(0)  # Convert image to tensor and add batch dimension\n",
    "    return image_tensor.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Load the unseen image\n",
    "image_path = \"test.jpg\"\n",
    "image_tensor = prepare_image(image_path)\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    prediction = model(image_tensor)\n",
    "\n",
    "# `prediction` contains:\n",
    "# - boxes: predicted bounding boxes\n",
    "# - labels: predicted class labels\n",
    "# - scores: predicted scores for each box (confidence level)\n",
    "COCO_CLASSES = {0: \"Background\", 1: \"Chair\", 2: \"Person\", 3: \"Table\"}\n",
    "\n",
    "def get_class_name(class_id):\n",
    "    return COCO_CLASSES.get(class_id, \"Unknown\")\n",
    "    \n",
    "# Draw bounding boxes with the correct class names and increase image size\n",
    "def draw_boxes(image, prediction, fig_size=(10, 10)):\n",
    "    boxes = prediction[0]['boxes'].cpu().numpy()  # Get predicted bounding boxes\n",
    "    labels = prediction[0]['labels'].cpu().numpy()  # Get predicted labels\n",
    "    scores = prediction[0]['scores'].cpu().numpy()  # Get predicted scores\n",
    "    \n",
    "    # Set a threshold for showing boxes (e.g., score > 0.5)\n",
    "    threshold = 0.5\n",
    "    \n",
    "    # Set up the figure size to control the image size\n",
    "    plt.figure(figsize=fig_size)  # Adjust the figure size here\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score > threshold:\n",
    "            x_min, y_min, x_max, y_max = box\n",
    "            class_name = get_class_name(label)  # Get the class name\n",
    "            plt.imshow(image)  # Display the image\n",
    "            plt.gca().add_patch(plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, \n",
    "                                              linewidth=2, edgecolor='r', facecolor='none'))\n",
    "            plt.text(x_min, y_min, f\"{class_name} ({score:.2f})\", color='r')\n",
    "    \n",
    "    plt.axis('off')  # Turn off axis\n",
    "    plt.show()\n",
    "\n",
    "# Display the image with bounding boxes and correct labels\n",
    "draw_boxes(Image.open(image_path), prediction, fig_size=(12, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
